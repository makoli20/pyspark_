{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "209fa021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"UberDataAnalysis\").getOrCreate()\n",
    "df = spark.read.csv(\"dataset.csv\", header=True, inferSchema=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a84a3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+------+---------------+--------+--------------+\n",
      "|     Date|Local_Time|Eyeballs|Zeroes|Completed_Trips|Requests|Unique_Drivers|\n",
      "+---------+----------+--------+------+---------------+--------+--------------+\n",
      "|10-Sep-12|         7|       5|     0|              2|       2|             9|\n",
      "|10-Sep-12|         8|       6|     0|              2|       2|            14|\n",
      "|10-Sep-12|         9|       8|     3|              0|       0|            14|\n",
      "|10-Sep-12|        10|       9|     2|              0|       1|            14|\n",
      "|10-Sep-12|        11|      11|     1|              4|       4|            11|\n",
      "|10-Sep-12|        12|      12|     0|              2|       2|            11|\n",
      "|10-Sep-12|        13|       9|     1|              0|       0|             9|\n",
      "|10-Sep-12|        14|      12|     1|              0|       0|             9|\n",
      "|10-Sep-12|        15|      11|     2|              1|       2|             7|\n",
      "|10-Sep-12|        16|      11|     2|              3|       4|             6|\n",
      "|10-Sep-12|        17|      12|     2|              3|       4|             4|\n",
      "|10-Sep-12|        18|      11|     1|              3|       4|             7|\n",
      "|10-Sep-12|        19|      13|     2|              2|       3|             7|\n",
      "|10-Sep-12|        20|      11|     1|              0|       0|             5|\n",
      "|10-Sep-12|        21|      11|     0|              1|       1|             3|\n",
      "|10-Sep-12|        22|      16|     3|              0|       2|             4|\n",
      "|10-Sep-12|        23|      21|     5|              3|       3|             4|\n",
      "|11-Sep-12|         0|       9|     3|              1|       1|             3|\n",
      "|11-Sep-12|         1|       3|     2|              0|       1|             3|\n",
      "|11-Sep-12|         2|       1|     1|              0|       0|             1|\n",
      "+---------+----------+--------+------+---------------+--------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Renaming the columns for usability \n",
    "\n",
    "column_mapping = {\n",
    "    \"Time (Local)\": \"Local_Time\",\n",
    "    \"Eyeballs \": \"Eyeballs\",\n",
    "    \"Zeroes \": \"Zeroes\",\n",
    "    \"Completed Trips \": \"Completed_Trips\",\n",
    "    \"Requests \": \"Requests\",\n",
    "    \"Unique Drivers\": \"Unique_Drivers\"\n",
    "}\n",
    "\n",
    "for old_col, new_col in column_mapping.items():\n",
    "    df = df.withColumnRenamed(old_col, new_col)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5da3dbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Local_Time: integer (nullable = true)\n",
      " |-- Eyeballs: integer (nullable = true)\n",
      " |-- Zeroes: integer (nullable = true)\n",
      " |-- Completed_Trips: integer (nullable = true)\n",
      " |-- Requests: integer (nullable = true)\n",
      " |-- Unique_Drivers: integer (nullable = true)\n",
      " |-- Timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Creating new 'timestamp' column\n",
    "\n",
    "df = df.withColumn(\"Timestamp\", from_unixtime(unix_timestamp(col(\"Date\"), \"yyyy-MM-dd\") + col(\"Local_Time\") * 3600))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64a8d6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22-Sep-12\n"
     ]
    }
   ],
   "source": [
    "# Which date had the most completed trips during the two-week period?\n",
    "from pyspark.sql.functions import max\n",
    "completed_trips_by_date = df.groupBy(\"Date\").sum(\"Completed_Trips\")\n",
    "\n",
    "date_with_most_completed_trips = completed_trips_by_date \\\n",
    "    .orderBy(\"sum(Completed_Trips)\", ascending=False) \\\n",
    "    .select(\"Date\") \\\n",
    "    .first()[\"Date\"]\n",
    "\n",
    "print(date_with_most_completed_trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97bcec44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+------+---------------+--------+--------------+-------------------+\n",
      "|      Date|Local_Time|Eyeballs|Zeroes|Completed_Trips|Requests|Unique_Drivers|          Timestamp|\n",
      "+----------+----------+--------+------+---------------+--------+--------------+-------------------+\n",
      "|2012-09-10|         7|       5|     0|              2|       2|             9|2012-09-10 07:00:00|\n",
      "|2012-09-10|         8|       6|     0|              2|       2|            14|2012-09-10 08:00:00|\n",
      "|2012-09-10|         9|       8|     3|              0|       0|            14|2012-09-10 09:00:00|\n",
      "|2012-09-10|        10|       9|     2|              0|       1|            14|2012-09-10 10:00:00|\n",
      "|2012-09-10|        11|      11|     1|              4|       4|            11|2012-09-10 11:00:00|\n",
      "|2012-09-10|        12|      12|     0|              2|       2|            11|2012-09-10 12:00:00|\n",
      "|2012-09-10|        13|       9|     1|              0|       0|             9|2012-09-10 13:00:00|\n",
      "|2012-09-10|        14|      12|     1|              0|       0|             9|2012-09-10 14:00:00|\n",
      "|2012-09-10|        15|      11|     2|              1|       2|             7|2012-09-10 15:00:00|\n",
      "|2012-09-10|        16|      11|     2|              3|       4|             6|2012-09-10 16:00:00|\n",
      "|2012-09-10|        17|      12|     2|              3|       4|             4|2012-09-10 17:00:00|\n",
      "|2012-09-10|        18|      11|     1|              3|       4|             7|2012-09-10 18:00:00|\n",
      "|2012-09-10|        19|      13|     2|              2|       3|             7|2012-09-10 19:00:00|\n",
      "|2012-09-10|        20|      11|     1|              0|       0|             5|2012-09-10 20:00:00|\n",
      "|2012-09-10|        21|      11|     0|              1|       1|             3|2012-09-10 21:00:00|\n",
      "|2012-09-10|        22|      16|     3|              0|       2|             4|2012-09-10 22:00:00|\n",
      "|2012-09-10|        23|      21|     5|              3|       3|             4|2012-09-10 23:00:00|\n",
      "|2012-09-11|         0|       9|     3|              1|       1|             3|2012-09-11 00:00:00|\n",
      "|2012-09-11|         1|       3|     2|              0|       1|             3|2012-09-11 01:00:00|\n",
      "|2012-09-11|         2|       1|     1|              0|       0|             1|2012-09-11 02:00:00|\n",
      "+----------+----------+--------+------+---------------+--------+--------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf1afecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest number of completed trips within a 24-hour period: 257\n"
     ]
    }
   ],
   "source": [
    "#What was the highest number of completed trips within a 24-hour period?\n",
    "\n",
    "df = df.withColumn(\"Timestamp\", col(\"Timestamp\").cast(\"timestamp\"))\n",
    "\n",
    "# Group the data by 24-hour windows and sum the completed trips\n",
    "completed_trips_by_window = df \\\n",
    "    .groupBy(window(\"Timestamp\", \"24 hours\")) \\\n",
    "    .agg(sum(\"Completed_Trips\").alias(\"Total_Completed_Trips\")) \\\n",
    "    .orderBy(\"Total_Completed_Trips\", ascending=False)\n",
    "\n",
    "# Get the highest number of completed trips within a 24-hour period\n",
    "highest_completed_trips_in_24_hours = completed_trips_by_window \\\n",
    "    .select(\"Total_Completed_Trips\") \\\n",
    "    .first()[\"Total_Completed_Trips\"]\n",
    "\n",
    "print(f\"Highest number of completed trips within a 24-hour period: {highest_completed_trips_in_24_hours}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aef13505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hour with the most requests is: 23\n"
     ]
    }
   ],
   "source": [
    "#Which hour of the day had the most requests during the two-week period?\n",
    "\n",
    "from pyspark.sql.functions import hour, sum\n",
    "\n",
    "hourly_requests = df \\\n",
    "    .groupBy(hour(\"Timestamp\").alias(\"hour\")) \\\n",
    "    .agg(sum(\"Requests\").alias(\"total_requests\")) \\\n",
    "    .orderBy(\"total_requests\", ascending=False)\n",
    "\n",
    "most_requested_hour = hourly_requests.select(\"hour\").first()[0]\n",
    "print(\"The hour with the most requests is:\", most_requested_hour)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b3be0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of zeros that occurred on weekends is: 29.111266620014 %\n"
     ]
    }
   ],
   "source": [
    "#What percentages of all zeroes during the two-week period occurred on weekends (Friday at 5 pm to Sunday at 3 am)?\n",
    "\n",
    "from pyspark.sql.functions import dayofweek, hour\n",
    "\n",
    "weekend_zeros = df.filter((hour(\"Timestamp\") >= 17) | (hour(\"Timestamp\") < 3)).filter((dayofweek(\"Date\") == 6) | (dayofweek(\"Date\") == 7)).agg(sum(\"Zeroes\").alias(\"weekend_zeros\")).collect()[0][\"weekend_zeros\"]\n",
    "\n",
    "total_zeros = df.agg(sum(\"Zeroes\").alias(\"total_zeros\")).collect()[0][\"total_zeros\"]\n",
    "\n",
    "percent_weekend_zeros = weekend_zeros / total_zeros * 100\n",
    "\n",
    "print(\"The percentage of zeros that occurred on weekends is:\", percent_weekend_zeros, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b877c745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weighted average ratio of completed trips per driver is: 0.8276707747535552\n"
     ]
    }
   ],
   "source": [
    "#What is the weighted average ratio of completed trips per driver during the two-week period?\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "weighted_avg = df.withColumn(\"completed_per_driver\", df[\"Completed_Trips\"] / df[\"Unique_Drivers\"]) \\\n",
    "                 .groupBy(\"Date\", \"Local_Time\") \\\n",
    "                 .agg(avg(\"completed_per_driver\").alias(\"avg_completed_per_driver\"), sum(\"Completed_Trips\").alias(\"total_completed_trips\")) \\\n",
    "                 .withColumn(\"weighted_ratio\", col(\"avg_completed_per_driver\") * col(\"total_completed_trips\")) \\\n",
    "                 .agg(sum(\"weighted_ratio\") / sum(\"total_completed_trips\")).collect()[0][0]\n",
    "\n",
    "print(\"The weighted average ratio of completed trips per driver is:\", weighted_avg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c3b87b",
   "metadata": {},
   "source": [
    "### The consecutive_sum column represents the sum of unique requests over an 8-hour consecutive window, including the current hour. In this case, the sum is 80, indicating the total number of unique requests observed over the 8 consecutive hours, where 8:00 PM is part of that window.\n",
    "### Businesses can use this insight for operational planning, resource allocation, or targeted promotions during this peak demand period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0529381f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/15 14:16:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/15 14:16:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/15 14:16:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/15 14:16:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/15 14:16:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+---------------+\n",
      "|hour|unique_requests|consecutive_sum|\n",
      "+----+---------------+---------------+\n",
      "|  20|             12|             80|\n",
      "+----+---------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/15 14:16:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/15 14:16:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/15 14:16:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/15 14:16:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "#In drafting a driver schedule in terms of 8 hours shifts, when are the busiest 8 consecutive hours over the two-week period in terms of unique requests? A new shift starts every 8 hours. Assume that a driver will work the same shift each day.\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col, hour, countDistinct\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "hourly_unique_requests = (df\n",
    "  .groupBy(hour(\"Timestamp\").alias(\"hour\"))\n",
    "  .agg(countDistinct(\"Requests\").alias(\"unique_requests\"))\n",
    ")\n",
    "\n",
    "window = Window.orderBy(col(\"unique_requests\").desc()).rowsBetween(0, 7)\n",
    "busiest_8_consecutive_hours = (hourly_unique_requests\n",
    "  .select(\"*\", sum(\"unique_requests\").over(window).alias(\"consecutive_sum\"))\n",
    "  .orderBy(col(\"consecutive_sum\").desc()) \n",
    "  .limit(1)\n",
    ")\n",
    "\n",
    "busiest_8_consecutive_hours.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957d0a63",
   "metadata": {},
   "source": [
    "### The recommendation is to add 5 drivers during the 2:00 AM hour. This suggestion is based on the analysis of the average number of requests per unique driver during that specific hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "06a493b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|Local_Time|requests_per_driver|\n",
      "+----------+-------------------+\n",
      "|         2|               20.0|\n",
      "+----------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#If you could add 5 drivers to any single hour of every day during the two-week period, which hour should you add them to? Hint: Consider both rider eyeballs and driver supply when choosing.\n",
    "\n",
    "from pyspark.sql.functions import sum, countDistinct, desc\n",
    "\n",
    "requests_per_driver = (df.groupBy('Local_Time').agg(\n",
    "    (sum('Requests') / countDistinct('Unique_Drivers')).alias('requests_per_driver'))\n",
    ")\n",
    "\n",
    "requests_per_driver.orderBy(desc('requests_per_driver')).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a99d805a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+------------------+\n",
      "|Local_Time|avg_completed_trips|avg_unique_drivers|\n",
      "+----------+-------------------+------------------+\n",
      "|         4|0.14285714285714285|0.6428571428571429|\n",
      "+----------+-------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Looking at the data from all two weeks, which time might make the most sense to consider a true \"end day\" instead of midnight? (i.e when are supply and demand at both their natural minimums)\n",
    "\n",
    "avg_trips_and_drivers = (df.groupBy('Local_Time').agg(avg('Completed_Trips').alias('avg_completed_trips'),avg('Unique_Drivers').alias('avg_unique_drivers')\n",
    "))\n",
    "\n",
    "avg_trips_and_drivers.orderBy('avg_completed_trips', 'avg_unique_drivers').show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9effcb92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d939cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
